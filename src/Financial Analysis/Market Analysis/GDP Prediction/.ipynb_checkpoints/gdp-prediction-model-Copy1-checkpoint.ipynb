{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaiCm2JYQnY6"
   },
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "WT18AgP1QnZA"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jz_-ngFkQnZC"
   },
   "source": [
    "## Initial Check on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "ykSkzIvCQnZD",
    "outputId": "f7522154-beca-4d41-ef25-1d30b0f7e8bc"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"world (1).csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIWfPm-TQnZE",
    "outputId": "63fc4d2b-fab2-4fd7-ff31-33c50b49836b"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jC9mf5yUQnZE"
   },
   "source": [
    "Dataset has 20 Columns with 227 Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "FpmDEU8pQnZF",
    "outputId": "d1c0d069-835e-479f-e646-08ed7845a610"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jr19pfNfQnZF"
   },
   "source": [
    "Only three columns are having proper numeric values. We can see in the previous table that most of the columns are having object as the datatype. This has to be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKfJQx-EQnZG"
   },
   "outputs": [],
   "source": [
    "## Changing the Datatype\n",
    "\n",
    "for col in ['Country', 'Region']:\n",
    "    df[col] = df[col].astype('category')\n",
    "    \n",
    "for col in ['Pop. Density (per sq. mi.)', 'Coastline (coast/area ratio)','Net migration','Infant mortality (per 1000 births)','Literacy (%)','Phones (per 1000)','Arable (%)','Crops (%)','Other (%)','Climate','Birthrate','Deathrate','Agriculture','Industry','Service']:\n",
    "    df[col] = df[col].astype('str')\n",
    "    df[col] = df[col].str.replace(\",\",\".\").astype(float)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3eoaUP5QnZH"
   },
   "source": [
    "Country and Region columns are converted to **Category** Datatype while rest of numeric data is converted to **float**. The category data type in pandas is a hybrid data type. It looks and behaves like a string in many instances but internally is represented by an array of integers. This allows the data to be sorted in a custom order and to more efficiently store the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QsJmlMi1QnZI",
    "outputId": "fab75aed-6dde-4005-8a01-915e4e1a3991"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "PDw14IE8QnZI",
    "outputId": "8e55d091-fe7b-4a0f-c2f4-b9acdba7b7de"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00YKFeZ5QnZJ"
   },
   "source": [
    "## Understanding more about the Dataset\n",
    "\n",
    "A few of the columns: Climate, Agriculture, Industry, and Service have not been explained exactly what they include as values. We need to understand it better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "aunPcXB3QnZJ",
    "outputId": "d5fe5ae8-17e3-4346-90d3-80bea8a78c36"
   },
   "outputs": [],
   "source": [
    "df.loc[:, ['Country', 'Region', 'Climate', 'Agriculture', 'Industry', 'Service']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mduR9JRgQnZJ"
   },
   "source": [
    "It looks like Agriculture , Industry and Service Columns represent the percent of Economy or GDP of a country that is being contributed by the respective economic activity. To understand Climate column, we can look at the distinct values and see which rows are coming together under the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JjHPpJVkQnZK",
    "outputId": "7f3071b1-f2a3-4c5f-ab47-560d50297e20"
   },
   "outputs": [],
   "source": [
    "df['Climate'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "id": "hpOmYJToQnZK",
    "outputId": "3593c1c1-08b1-40f1-b49c-e870b319fea5"
   },
   "outputs": [],
   "source": [
    "h = {}\n",
    "for cat in [1, 2, 3, 4, 1.5, 2.5]:\n",
    "    h[cat] = df.loc[:, ['Country', 'Region', 'Climate']][df['Climate'] == cat].head()\n",
    "\n",
    "pd.concat([h[1], h[2], h[3], h[4], h[1.5], h[2.5]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uE-3rWIWQnZK"
   },
   "source": [
    "A guess for what the categories are pointing to is:\n",
    "\n",
    "**1**   - Countries that are desert kind/hot. \\\n",
    "**1.5** - Countries that are both hot and tropical. \\\n",
    "**2**   - Countries with a tropical climate.\\\n",
    "**2.5** - Countries that are both cold and tropical.\\\n",
    "**3**   - Countries with cold Climate.\\\n",
    "**4**   - These countries also seem to have cold climate. Not mentioned why it is separated from Category 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KL4w-_-hQnZL"
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "DFYC9aYwQnZL",
    "outputId": "2c53b4df-3f84-4b46-dd66-5818599e8a65"
   },
   "outputs": [],
   "source": [
    "## Finding the Null Value in each Column Percentage\n",
    "\n",
    "num_missing = df.isnull().sum()\n",
    "missing_value_df = pd.DataFrame({'Column_name': df.columns,'num_missing': num_missing})\n",
    "missing_value_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUqB-2Z2QnZL"
   },
   "source": [
    "There is a very little percentage of data in each column that is missing. We can view it in a heatmap to get a different visual analysis of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "g_b-0lKVQnZM",
    "outputId": "8f7dcf59-b6d5-4208-e8a7-ae3cd746ca44"
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11,8)})\n",
    "sns.heatmap(df.isnull()).set(title = 'Missing Data', xlabel = 'Columns', ylabel = 'Data Points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_hKZIUZQnZM"
   },
   "source": [
    "It is seen that there are significantly low values of **NULL** in some of the columns : **{\"Net Migration\", \"Infant Mortality\", \"GDP\", \"Literacy\", \"Phones\", \"Arable\", \"Crops\", \"Other\", \"Climate\", \"Birthrate\", \"Deathrate\", \"Agriculture\", \"Industry\", \"Service\"}**. The Rows with these values can be dealt with later for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "m_Tfl06OQnZM",
    "outputId": "b7e777ed-0a4a-4bae-a919-c3761700a28f"
   },
   "outputs": [],
   "source": [
    "## Checking Rows in which null values are present for each column\n",
    "\n",
    "df1 = df[df['Net migration'].isna()]\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIwkWZlkQnZN"
   },
   "source": [
    "## Changes suggested for these Rows with NaN values\n",
    "\n",
    "| Feature   |      Number of missing Values    |  Change |\n",
    "|:----------|:-------------:|------:|\n",
    "| Net migration | 3 |  Belong to very small nations. Change to 0.|\n",
    "| Infant mortality (per 1000 births) |  3   |Belong to very small nations. Change to 0.  |\n",
    "| GDP ($ per capita) | 1| From Google search, it is \\$2500. Change to same.|\n",
    "|Literacy (\\%)|18| Replace by the mean literacy of each missing value's region|\n",
    "|Phones (per 1000)|4|Replace by the mean phones of each missing value's region|\n",
    "|Arable (\\%)|2|Very small islands.Change to 0.|\n",
    "|Crops (\\%)|2|Very small islands.Change to 0.|\n",
    "|Other (\\%)|2|Very small islands.Change to 0.|\n",
    "|Climate|22|Change to 0. It represents \"unknown\" category.|\n",
    "|Birthrate|3|Replace with their region's mean rates|\n",
    "|Deathrate|4|Replace with their region's mean rates|\n",
    "|Agriculture|15|Calculated guess seeing how similar countries have. Change to 0.15.|\n",
    "|Industry|16|Calculated guess seeing how similar countries have. Change to 0.05.|\n",
    "|Service|15|Calculated guess seeing how similar countries have. Change to 0.8.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2rkTMIVQnZN"
   },
   "outputs": [],
   "source": [
    "change1 = [(\"Net migration\", 0), (\"Infant mortality (per 1000 births)\", 0), (\"GDP ($ per capita)\", 2500), (\"Arable (%)\", 0), (\"Crops (%)\", 0),(\"Other (%)\",0),(\"Climate\",0),(\"Agriculture\",0.15), (\"Industry\", 0.05), (\"Service\", 0.8) ]\n",
    "for col in change1:\n",
    "    df[col[0]].fillna(col[1], inplace = True)\n",
    "    \n",
    "change2 = [\"Literacy (%)\", \"Phones (per 1000)\", \"Birthrate\", \"Deathrate\"]\n",
    "for col in change2:\n",
    "    df[col].fillna(df.groupby('Region')[col].transform('mean'), inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofaOxZUhQnZN",
    "outputId": "3c20e99a-8bab-4f4d-f4fd-2dd7d275e0dd"
   },
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wt5V1BZjQnZN"
   },
   "source": [
    "# EDA\n",
    "## Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1FllGHxmQnZN",
    "outputId": "9371de64-d9ac-46d4-9a01-50ca7378920a"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,16)) \n",
    "\n",
    "# Only take numeric columns to avoid string-to-float conversion errors\n",
    "numeric_df = df.select_dtypes(include='number')\n",
    "\n",
    "sns.heatmap(numeric_df.corr(), annot=True, ax=ax, cmap='Spectral').set(\n",
    "    title='Feature Correlation', xlabel='Columns', ylabel='Columns'\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RyIMVFTQnZO"
   },
   "source": [
    "## Insights\n",
    "**Expected Strong Correlation between :** \n",
    "1. Infant mortality and Birthrate \n",
    "2. Gdp per capita and Phones\n",
    "\n",
    "**Expected Strong Anticorrelation between:**\n",
    "1. Infant mortality and Literacy\n",
    "2. Arable and Other \n",
    "3. Birthrate and Literacy\n",
    "\n",
    "**Unexpected Strong Correlation between:**\n",
    "1. Infant mortality and Agriculture\n",
    "\n",
    "**Unexpected Strong Anticorrelation between:**\n",
    "1. Birthrate and Phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 724
    },
    "id": "3z42zfXUQnZO",
    "outputId": "34af7d99-1049-467a-add4-04d0dcb2aad5"
   },
   "outputs": [],
   "source": [
    "f = sns.pairplot(df[['Population', 'Area (sq. mi.)', 'Net migration', 'GDP ($ per capita)', 'Climate']], hue = \"Climate\")\n",
    "f.fig.suptitle('Feature Relations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVei5kJHQnZP"
   },
   "source": [
    "There is a fair correlation between GDP and migration, which makes sense, since migrants tend to move to countries with better opportunities and higher GDP per capita."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CguYOopKQnZP"
   },
   "source": [
    "## Regional Analysis\n",
    "\n",
    "Checking the number of Countries in each region, the GDP per capita, population and migration to get some insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BAIsZpu6QnZP",
    "outputId": "aa291f04-fd2b-451c-a696-6e51e12bf2fc"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 20))\n",
    "plt.title('Regional Analysis')\n",
    "ax1 = fig.add_subplot(4, 1, 1)\n",
    "ax2 = fig.add_subplot(4, 1, 2)\n",
    "ax3 = fig.add_subplot(4, 1, 3)\n",
    "ax4 = fig.add_subplot(4, 1, 4)\n",
    "sns.countplot(data= df, y= 'Region', ax= ax1, palette=\"flare\")\n",
    "sns.barplot(data= df, y= 'Region', x= 'GDP ($ per capita)', ax= ax2, palette=\"flare\", ci= None)\n",
    "sns.barplot(data= df, y= 'Region', x= 'Net migration', ax= ax3, palette=\"flare\", ci= None)\n",
    "sns.barplot(data= df, y= 'Region', x= 'Population', ax= ax4, palette=\"flare\", ci= None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcXIiZ_XQnZP"
   },
   "source": [
    "## Insights\n",
    "1. Sub-Saharan Africa and Latin America & Caribbean regions have the most countries.\n",
    "2. Western Europe and North America have the highest GDP per capita, while Sub-Saharan Africa has the lowest GDP per capita.\n",
    "3. Asia, North America, and North Europe, are the main regions where migrants from other regions go to.\n",
    "4. Asia has the largest population, Oceania has the smallest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTaONuIrQnZQ"
   },
   "source": [
    "# GDP Analysis\n",
    "\n",
    "The relation between GDP and Infant Mortality rate, Literacy, Arable Land is studied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VPnzLm24QnZQ",
    "outputId": "10b0e6e1-0d09-4916-9fb2-5a6a723a6624"
   },
   "outputs": [],
   "source": [
    "sns.jointplot(data= df, x= 'Literacy (%)', y= 'GDP ($ per capita)', kind= \"hist\",color='coral')\n",
    "sns.jointplot(data= df, x= 'Arable (%)', y= 'GDP ($ per capita)', kind= \"hist\", color='coral')\n",
    "sns.jointplot(data= df, x= 'Infant mortality (per 1000 births)', y= 'GDP ($ per capita)', kind= \"hist\",color='coral')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgwEPi_iQnZQ"
   },
   "source": [
    "## Analysis \n",
    "\n",
    "1. Higher the country's GDP, the more literate the population is, and vice versa.\n",
    "2. No clear relationship between GDP and \\% of Arable land. It shows that Agriculture is not the strongest factor economically.\n",
    "3. Poor countries suffer more from Infant mortality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vf3IMfuHQnZQ"
   },
   "source": [
    "## Data Pre conditioning\n",
    "\n",
    "1. Transform 'Region' column into numerical values.\n",
    "2. Split data set into Training and Testing parts (80/20).\n",
    "3. Trying to analyse (with/without Feature Selection, with/without Feature Scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahk71A6wQnZQ"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V_5HIvUEQnZQ",
    "outputId": "e6411bcf-792e-4421-c2ba-c96f315287dd"
   },
   "outputs": [],
   "source": [
    "# Transporming Region\n",
    "\n",
    "df_final = pd.concat([df,pd.get_dummies(df['Region'], prefix='Region')], axis=1).drop(['Region'],axis=1)\n",
    "print(df_final.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tFdFlrUQnZR"
   },
   "source": [
    "Now it has 227 entries and 30 Columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6wH8ttFQnZR"
   },
   "outputs": [],
   "source": [
    "# Without scaling , the full dataset\n",
    "y = df_final[\"GDP ($ per capita)\"]\n",
    "X = df_final.drop([\"GDP ($ per capita)\",'Country'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "# With Scaling\n",
    "sc_X = StandardScaler()\n",
    "X2_train = sc_X.fit_transform(X_train)\n",
    "X2_test = sc_X.fit_transform(X_test)\n",
    "y2_train = y_train\n",
    "y2_test = y_test\n",
    "\n",
    "# Without scaling, Feature selected Dataset (corr > +/-0.3)\n",
    "y3 = y\n",
    "X3 = df_final.drop(['GDP ($ per capita)','Country','Population', 'Area (sq. mi.)', 'Coastline (coast/area ratio)', 'Arable (%)',\n",
    "                      'Crops (%)', 'Other (%)', 'Climate', 'Deathrate', 'Industry'], axis=1)\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.2, random_state=101)\n",
    "\n",
    "\n",
    "# With scaling\n",
    "sc_X4 = StandardScaler()\n",
    "X4_train = sc_X4.fit_transform(X3_train)\n",
    "X4_test = sc_X4.fit_transform(X3_test)\n",
    "y4_train = y3_train\n",
    "y4_test = y3_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_x4anf-tQnZR"
   },
   "source": [
    "# Linear Regression \n",
    "\n",
    "Basic Regression Technique is seen first to see if any linear relationship exists. Model Training is done for all 4 datasets, predictions are done and it is Evaluated to see if any improvement is seen with Feature Selection or Feature Scaling. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r_GP07S3QnZR",
    "outputId": "2fad587d-dcd5-465f-ee70-b750f8fc72f3"
   },
   "outputs": [],
   "source": [
    "# Model Training\n",
    "lm1 = LinearRegression()\n",
    "lm1.fit(X_train,y_train)\n",
    "\n",
    "lm2 = LinearRegression()\n",
    "lm2.fit(X2_train,y2_train)\n",
    "\n",
    "lm3 = LinearRegression()\n",
    "lm3.fit(X3_train,y3_train)\n",
    "\n",
    "lm4 = LinearRegression()\n",
    "lm4.fit(X4_train,y4_train)\n",
    "\n",
    "# Predictions\n",
    "lm1_pred = lm1.predict(X_test)\n",
    "lm2_pred = lm2.predict(X2_test)\n",
    "lm3_pred = lm3.predict(X3_test)\n",
    "lm4_pred = lm4.predict(X4_test)\n",
    "\n",
    "# Evaluation Function \n",
    "def eval(cond, y, pred):\n",
    "    print(cond)\n",
    "    print(\"____________________________\\n\")\n",
    "    print('MAE:', metrics.mean_absolute_error(y, pred))\n",
    "    print('RMSE:', np.sqrt(metrics.mean_squared_error(y, pred)))\n",
    "    print('R2_Score: ', metrics.r2_score(y, pred))\n",
    "    print(\"*****************************\\n\\n\")\n",
    "    \n",
    "eval(\"All features, No scaling:\",y_test,lm1_pred)\n",
    "eval(\"\\nAll features, with scaling:\",y2_test,lm2_pred)\n",
    "eval(\"\\nSelected features, No scaling:\",y3_test,lm3_pred)\n",
    "eval(\"\\nSelected features, with scaling:\",y4_test,lm4_pred)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3GnftrdQnZR"
   },
   "source": [
    "## Analysis\n",
    "1. **Feature Selection** helps in reducing the errors. It is needed for this model.\n",
    "2. **Feature Scaling** did not have that significant effect on the prediction performance. \n",
    "3. Decent predictions obtained with both **Selection** and **Scaling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4UyB3dTQnZS"
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dez0TH7nQnZS",
    "outputId": "7320a714-4d54-466f-f7dc-f6c5ed454771"
   },
   "outputs": [],
   "source": [
    "# Model Training\n",
    "svm1 = SVR(kernel='rbf')\n",
    "svm1.fit(X_train,y_train)\n",
    "\n",
    "svm2 = SVR(kernel='rbf')\n",
    "svm2.fit(X2_train,y2_train)\n",
    "\n",
    "svm3 = SVR(kernel='rbf')\n",
    "svm3.fit(X3_train,y3_train)\n",
    "\n",
    "svm4 = SVR(kernel='rbf')\n",
    "svm4.fit(X4_train,y4_train)\n",
    "\n",
    "# Predictions\n",
    "svm1_pred = svm1.predict(X_test)\n",
    "svm2_pred = svm2.predict(X2_test)\n",
    "svm3_pred = svm3.predict(X3_test)\n",
    "svm4_pred = svm4.predict(X4_test)\n",
    "\n",
    "# Evaluation\n",
    "eval(\"All features, No scaling:\",y_test,svm1_pred)\n",
    "eval(\"\\nAll features, with scaling:\",y2_test,svm2_pred)\n",
    "eval(\"\\nSelected features, No scaling:\",y3_test,svm3_pred)\n",
    "eval(\"\\nSelected features, with scaling:\",y4_test,svm4_pred)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihOEfI6PQnZS"
   },
   "source": [
    "## Analysis\n",
    "\n",
    "1. **Feature Scaling** and **Feature Selection**, made almost no difference in the prediction performance of the SVM algorithm.\n",
    "\n",
    "2. The results of **SVM is worse than LR**.\n",
    "\n",
    "## Optimising SVM\n",
    "Using **Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Z83-u_pQnZS",
    "outputId": "1da3427c-49e4-4619-e840-c1416f9f2a28"
   },
   "outputs": [],
   "source": [
    "param_grid = {'C': [1, 10, 100], 'gamma': [0.01,0.001,0.0001], 'kernel': ['rbf']} \n",
    "grid = GridSearchCV(SVR(),param_grid,refit=True,verbose=3)\n",
    "grid.fit(X4_train,y4_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rkiXs6J4QnZT",
    "outputId": "d3a2102f-1cb7-405d-d0c0-bea352baf3f1"
   },
   "outputs": [],
   "source": [
    "print(\"Best Parameters are : {}\".format(grid.best_params_))\n",
    "print(\"Best Estimators are : {}\".format(grid.best_estimator_))\n",
    "grid_predictions = grid.predict(X4_test)\n",
    "eval(\"\\nSelected features, with scaling:\",y4_test,grid_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1m_iXAQYQnZT"
   },
   "source": [
    "It has **improved but performance is still lower** than LR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRE3ywhcQnZT"
   },
   "source": [
    "## Random Forest\n",
    "\n",
    "Scaling doesn't work in this model so it is not analysed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yef31pt_QnZT",
    "outputId": "a15e8d54-0fba-4e1e-c955-834bc97a11f2"
   },
   "outputs": [],
   "source": [
    "# Model Training\n",
    "rf1 = RandomForestRegressor(random_state=101, n_estimators=200)\n",
    "rf3 = RandomForestRegressor(random_state=101, n_estimators=200)\n",
    "rf1.fit(X_train, y_train)\n",
    "rf3.fit(X3_train, y3_train)\n",
    "\n",
    "# Prediction\n",
    "rf1_pred = rf1.predict(X_test)\n",
    "rf3_pred = rf3.predict(X3_test)\n",
    "\n",
    "# Evaluation\n",
    "eval(\"All features, No scaling:\",y_test,rf1_pred)\n",
    "eval(\"\\nSelected features, No scaling:\",y3_test,rf3_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxmXhbRgQnZT"
   },
   "source": [
    "## Optimising Random Forest\n",
    "\n",
    "**Grid Search** will be used to get optimal parameters. Only parameters chosen are n-estimators, min_samples_leaf, max_features, bootstrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EHC8rPkwQnZT",
    "outputId": "aceef341-b406-401d-9f8f-c21c2065b2a7"
   },
   "outputs": [],
   "source": [
    "## Choosing params\n",
    "rf_param_grid = {'max_features': ['sqrt', 'auto'],\n",
    "              'min_samples_leaf': [1, 3, 5],\n",
    "              'n_estimators': [100, 500, 1000],\n",
    "             'bootstrap': [False, True]} \n",
    "\n",
    "rf_grid = GridSearchCV(estimator= RandomForestRegressor(), param_grid = rf_param_grid,  n_jobs=-1, verbose=0)\n",
    "rf_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63l1pvEXQnZT",
    "outputId": "2df91249-c966-46c3-cbe8-7f66f549cd37"
   },
   "outputs": [],
   "source": [
    "print(\"Best Parameters are : {}\".format(rf_grid.best_params_))\n",
    "print(\"Best Estimators are : {}\".format(rf_grid.best_estimator_))\n",
    "rf_grid_predictions = rf_grid.predict(X_test)\n",
    "eval(\"\\nAll features, no scaling:\",y_test,rf_grid_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0HBNIvZQnZU"
   },
   "source": [
    "## Analysis\n",
    "\n",
    "1. Optimization process on RF regressor **has not changed the performance** in a significant manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ec_L_jhaQnZU"
   },
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Hc5bVspQnZU",
    "outputId": "5209efb3-8ecf-48f0-a472-b4da5c6f3a34"
   },
   "outputs": [],
   "source": [
    "# Model training\n",
    "gbm1 = GradientBoostingRegressor(learning_rate=0.1, n_estimators=100, min_samples_split=2, min_samples_leaf=1, max_depth=3,\n",
    "                                 subsample=1.0, max_features= None, random_state=101)\n",
    "gbm3 = GradientBoostingRegressor(learning_rate=0.1, n_estimators=100, min_samples_split=2, min_samples_leaf=1, max_depth=3,\n",
    "                                 subsample=1.0, max_features= None, random_state=101)\n",
    "\n",
    "gbm1.fit(X_train, y_train)\n",
    "gbm3.fit(X3_train, y3_train)\n",
    "\n",
    "# Prediction\n",
    "gbm1_pred = gbm1.predict(X_test)\n",
    "gbm3_pred = gbm3.predict(X3_test)\n",
    "\n",
    "# Evaluation\n",
    "eval(\"All features, No scaling:\",y_test,gbm1_pred)\n",
    "eval(\"\\nSelected features, No scaling:\",y3_test,gbm3_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uwhN77WQnZU"
   },
   "source": [
    "# Optimising GBM\n",
    "Grid Search will be used to get optimal parameters. Only parameters chosen are n-estimators , learning_rate , max_depth , subsample , min_samples_leaf , min_samples_split , max_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQcL7VvqQnZU",
    "outputId": "a19553ac-5760-4a60-c92c-faa64e34e87b"
   },
   "outputs": [],
   "source": [
    "## Choosing params\n",
    "gbm_param_grid = {'learning_rate':[1,0.1, 0.01, 0.001], \n",
    "           'n_estimators':[100, 500, 1000],\n",
    "          'max_depth':[3, 5, 8],\n",
    "          'subsample':[0.7, 1], \n",
    "          'min_samples_leaf':[1, 20],\n",
    "          'min_samples_split':[10, 20],\n",
    "          'max_features':[4, 7]}\n",
    "\n",
    "gbm_tuning = GridSearchCV(estimator =GradientBoostingRegressor(random_state=101),\n",
    "                          param_grid = gbm_param_grid,\n",
    "                          n_jobs=-1,\n",
    "                          cv=5)\n",
    "gbm_tuning.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQugfyk-QnZV",
    "outputId": "bc27f255-d7e9-479f-bc4e-60b8fff37e56"
   },
   "outputs": [],
   "source": [
    "print(\"Best Parameters are : {}\".format(gbm_tuning.best_params_))\n",
    "print(\"Best Estimators are : {}\".format(gbm_tuning.best_estimator_))\n",
    "gbm_grid_predictions = gbm_tuning.predict(X_test)\n",
    "eval(\"\\nAll features, no scaling:\",y_test,gbm_grid_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCcqZZGbQnZV"
   },
   "source": [
    "## Analysis\n",
    "\n",
    "1. Gradient Boosting **gave a good performance** even before Optimisation.\n",
    "2. Grid search **actually decreased the GBM performance** a bit. In general, we can say that GBM has a similar performance to that of Random Forest on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 899
    },
    "id": "cD1ckKgXQnZV",
    "outputId": "00e5e201-95df-4272-fe46-cf326afec516"
   },
   "outputs": [],
   "source": [
    "## Conclusion Plots \n",
    "fig, axs = plt.subplots(3, 2, figsize=(16,15))\n",
    "axs[0, 0].scatter(y4_test,lm4_pred,color='coral', linewidths=2, edgecolors='k')\n",
    "axs[0, 0].set_title('Linear Regression Prediction Performance (features selected and scaled)')\n",
    "axs[0, 1].scatter(y4_test,grid_predictions,color='coral', linewidths=2, edgecolors='k')\n",
    "axs[0, 1].set_title('Optimized SVM prediction Performance (with feature selection, and scaling)')\n",
    "axs[1, 0].scatter(y_test,rf1_pred,color='coral', linewidths=2, edgecolors='k')\n",
    "axs[1, 0].set_title('Random Forest prediction Performance (No feature selection)')\n",
    "axs[1, 1].scatter(y_test,rf_grid_predictions,color='coral', linewidths=2, edgecolors='k')\n",
    "axs[1, 1].set_title('Optimized Random Forest prediction Performance (No feature selection)')\n",
    "axs[2, 0].scatter(y_test,gbm1_pred,color='coral', linewidths=2, edgecolors='k')\n",
    "axs[2, 0].set_title('Gradient Boosting prediction Performance (No feature selection)')\n",
    "axs[2, 1].scatter(y_test,gbm_grid_predictions,color='coral', linewidths=2, edgecolors='k')\n",
    "axs[2, 1].set_title('Optimized Gradient Boosting prediction Performance')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='True GDP per Capita', ylabel='Predictions')\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avGdZ2JrQnZV"
   },
   "source": [
    "**Random Forest shows the best prediction performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZ-xrV1xQnZV"
   },
   "source": [
    "## Feature Importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "id": "f6DwXgK5QnZV",
    "outputId": "5180e96d-78e4-4d2d-f8c9-9adb349e45bb"
   },
   "outputs": [],
   "source": [
    "gbm_opt = GradientBoostingRegressor(learning_rate=0.01, n_estimators=500,max_depth=5, min_samples_split=10, min_samples_leaf=1, \n",
    "                                    subsample=0.7,max_features=7, random_state=101)\n",
    "gbm_opt.fit(X_train,y_train)\n",
    "feat_imp2 = pd.Series(gbm_opt.feature_importances_, list(X_train)).sort_values(ascending=False)\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "feat_imp2.plot(kind='bar', title='Importance of Features (Optimized)', color= 'skyblue')\n",
    "plt.ylabel('Feature Importance Score')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcI_N4PHQnZV"
   },
   "source": [
    "## Analysis\n",
    "\n",
    "1. This shows significant importance shown by some features like Phones, Agriculture, Infant mortality etc.\n",
    "2. Comparatively, the importance of Arable or Area is very less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCRmGYSmQnZW"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "4 different learning regressors **(Linear Regression, SVM, Random Forest, and Gradiant Boosting)** were tested to predict GDP, and the best prediction performance was seen in the order : \\\n",
    "\\\n",
    "**Random Forest > Gradiant Boosting > Linear Regression > SVM**\n",
    "\n",
    "The Metrics for the best prediction performance using Random Forest regressor, using all features in the dataset is:\n",
    "\n",
    "1. MAE: 2125.24\n",
    "2. RMSE: 3051.71\n",
    "3. R2_Score:  0.8873"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hKxis8mQnZW"
   },
   "outputs": [],
   "source": [
    "# model save for deployment\n",
    "joblib.dump(rf3,\"random_forest_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "gdp-prediction-model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
