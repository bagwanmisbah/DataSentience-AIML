{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166e83b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# LSTM-Only Advanced Deep Learning Pipeline with Preprocessing\n",
    "# ==============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Fix randomness\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea5ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# Preprocessing Functions\n",
    "# ==============================\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "\n",
    "    # Remove emojis\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "\n",
    "    # Remove non-alphabetical characters (keep .,!? for sentence structure)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s.,!?]\", \" \", text)\n",
    "\n",
    "    # Collapse multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def preprocess_dataframe(df, text_col, remove_value='\"'):\n",
    "    # Drop rows with unwanted text (like just a \" )\n",
    "    df = df[df[text_col].astype(str).str.strip() != remove_value]\n",
    "\n",
    "    # Apply cleaning\n",
    "    df[text_col] = df[text_col].apply(clean_text)\n",
    "\n",
    "    # Drop empty rows after cleaning\n",
    "    df = df[df[text_col].str.strip() != \"\"]\n",
    "\n",
    "    return df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d61303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# TextClassifier Class\n",
    "# ==============================\n",
    "class TextClassifier:\n",
    "    def __init__(self, max_features=20000, max_length=200):\n",
    "        self.max_features = max_features\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = None\n",
    "        self.models = {}\n",
    "        self.histories = {}\n",
    "\n",
    "    def handle_imbalanced_data(self, X_train, y_train, method='balanced_weights'):\n",
    "        print(f\"Original distribution: 0={sum(y_train==0)}, 1={sum(y_train==1)}\")\n",
    "        if method == 'balanced_weights':\n",
    "            pos, neg = sum(y_train==1), sum(y_train==0)\n",
    "            weight_ratio = np.sqrt(neg/pos)\n",
    "            class_weight = {0: 1.0, 1: min(weight_ratio, 5.0)}\n",
    "            print(f\"Using class weights: {class_weight}\")\n",
    "            return X_train, y_train, class_weight\n",
    "        else:\n",
    "            return X_train, y_train, None\n",
    "\n",
    "    def prepare_sequences(self, X_train, X_val, X_test):\n",
    "        self.tokenizer = Tokenizer(num_words=self.max_features, oov_token=\"<OOV>\")\n",
    "        self.tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "        def pad(X): \n",
    "            return pad_sequences(self.tokenizer.texts_to_sequences(X),\n",
    "                                 maxlen=self.max_length, padding='post')\n",
    "\n",
    "        return pad(X_train), pad(X_val), pad(X_test)\n",
    "\n",
    "    def create_improved_lstm_model(self, embedding_dim=128):\n",
    "        model = Sequential([\n",
    "            Embedding(self.max_features, embedding_dim, input_length=self.max_length),\n",
    "            Dropout(0.2),\n",
    "            LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),\n",
    "            BatchNormalization(),\n",
    "            LSTM(32, dropout=0.2, recurrent_dropout=0.2),\n",
    "            BatchNormalization(),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    def focal_loss(self, gamma=2., alpha=0.25):\n",
    "        def focal_loss_fixed(y_true, y_pred):\n",
    "            eps = tf.keras.backend.epsilon()\n",
    "            y_pred = tf.clip_by_value(y_pred, eps, 1. - eps)\n",
    "            p_t = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "            alpha_t = tf.where(tf.equal(y_true, 1), alpha, 1 - alpha)\n",
    "            return tf.reduce_mean(-alpha_t * tf.pow(1 - p_t, gamma) * tf.math.log(p_t))\n",
    "        return focal_loss_fixed\n",
    "\n",
    "    def train_model(self, model, X_train, X_val, y_train, y_val,\n",
    "                    model_name, class_weight=None, use_focal_loss=False, epochs=10):\n",
    "        print(f\"\\nTraining {model_name}...\")\n",
    "        loss_fn = self.focal_loss() if use_focal_loss else \"binary_crossentropy\"\n",
    "        if use_focal_loss: print(\"Using Focal Loss\")\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=5e-4),\n",
    "            loss=loss_fn,\n",
    "            metrics=[\"accuracy\", tf.keras.metrics.Precision(name=\"precision\"),\n",
    "                     tf.keras.metrics.Recall(name=\"recall\")]\n",
    "        )\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor=\"val_loss\", patience=3, factor=0.5, min_lr=1e-7),\n",
    "            ModelCheckpoint(f\"{model_name.lower()}_best.keras\",\n",
    "                            save_best_only=True, monitor=\"val_loss\", mode=\"min\")\n",
    "        ]\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=32 if class_weight else 64,\n",
    "            class_weight=class_weight,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        return model, history\n",
    "\n",
    "    def train_lstm_models(self, X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "                          balance_method=\"balanced_weights\"):\n",
    "        X_train_bal, y_train_bal, class_weight = self.handle_imbalanced_data(X_train, y_train, balance_method)\n",
    "        X_train_pad, X_val_pad, X_test_pad = self.prepare_sequences(X_train_bal, X_val, X_test)\n",
    "\n",
    "        configs = {\"LSTM_Focal\": (self.create_improved_lstm_model, True)}\n",
    "\n",
    "        results = []\n",
    "        for name, (fn, focal) in configs.items():\n",
    "            model = fn()\n",
    "            model.build(input_shape=(None, self.max_length))\n",
    "            model.summary()\n",
    "            trained, hist = self.train_model(model, X_train_pad, X_val_pad, y_train_bal, y_val,\n",
    "                                             name, class_weight if not focal else None, focal)\n",
    "            self.models[name], self.histories[name] = trained, hist\n",
    "\n",
    "            probs = trained.predict(X_test_pad).flatten()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "            res = {\"Model\": name, \"Accuracy\": None, \"F1\": None, \"ROC_AUC\": None}\n",
    "            if y_test is not None:\n",
    "                res[\"Accuracy\"] = (preds == y_test).mean()\n",
    "                res[\"F1\"] = f1_score(y_test, preds)\n",
    "                res[\"ROC_AUC\"] = roc_auc_score(y_test, probs)\n",
    "            results.append(res)\n",
    "\n",
    "        return pd.DataFrame(results), X_test_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25a5a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after cleaning: (159552, 3)\n",
      "Test shape after cleaning: (152781, 2)\n",
      "Original distribution: 0=115406, 1=12235\n",
      "Using class weights: {0: 1.0, 1: 3.0712290529008253}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prana\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m2,560,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,626,433</span> (10.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,626,433\u001b[0m (10.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,626,241</span> (10.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,626,241\u001b[0m (10.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM_Focal...\n",
      "Using Focal Loss\n",
      "Epoch 1/10\n",
      "\u001b[1m1995/1995\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m880s\u001b[0m 435ms/step - accuracy: 0.9022 - loss: 0.0343 - precision: 0.1003 - recall: 0.0025 - val_accuracy: 0.9041 - val_loss: 0.0301 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 5.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m1995/1995\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1140s\u001b[0m 544ms/step - accuracy: 0.9041 - loss: 0.0309 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.9041 - val_loss: 0.0299 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 5.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m1995/1995\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m965s\u001b[0m 484ms/step - accuracy: 0.9047 - loss: 0.0301 - precision: 0.8350 - recall: 0.0070 - val_accuracy: 0.9058 - val_loss: 0.0296 - val_precision: 0.8462 - val_recall: 0.0216 - learning_rate: 5.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m1995/1995\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m798s\u001b[0m 400ms/step - accuracy: 0.9140 - loss: 0.0224 - precision: 0.8553 - recall: 0.1242 - val_accuracy: 0.9570 - val_loss: 0.0128 - val_precision: 0.8657 - val_recall: 0.6532 - learning_rate: 5.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m1995/1995\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m718s\u001b[0m 360ms/step - accuracy: 0.9489 - loss: 0.0125 - precision: 0.9113 - recall: 0.5174 - val_accuracy: 0.9590 - val_loss: 0.0109 - val_precision: 0.9293 - val_recall: 0.6192 - learning_rate: 5.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1995/1995\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m686s\u001b[0m 344ms/step - accuracy: 0.9562 - loss: 0.0105 - precision: 0.9274 - recall: 0.5890 - val_accuracy: 0.9589 - val_loss: 0.0113 - val_precision: 0.9335 - val_recall: 0.6149 - learning_rate: 5.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1995/1995\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m749s\u001b[0m 375ms/step - accuracy: 0.9600 - loss: 0.0096 - precision: 0.9327 - recall: 0.6283 - val_accuracy: 0.9606 - val_loss: 0.0111 - val_precision: 0.9195 - val_recall: 0.6460 - learning_rate: 5.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m1995/1995\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m757s\u001b[0m 380ms/step - accuracy: 0.9641 - loss: 0.0085 - precision: 0.9380 - recall: 0.6697 - val_accuracy: 0.9595 - val_loss: 0.0124 - val_precision: 0.9226 - val_recall: 0.6309 - learning_rate: 5.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1995/1995\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m772s\u001b[0m 387ms/step - accuracy: 0.9687 - loss: 0.0073 - precision: 0.9432 - recall: 0.7169 - val_accuracy: 0.9616 - val_loss: 0.0134 - val_precision: 0.8903 - val_recall: 0.6842 - learning_rate: 2.5000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1995/1995\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m755s\u001b[0m 378ms/step - accuracy: 0.9720 - loss: 0.0066 - precision: 0.9504 - recall: 0.7468 - val_accuracy: 0.9616 - val_loss: 0.0141 - val_precision: 0.8872 - val_recall: 0.6865 - learning_rate: 2.5000e-04\n",
      "\u001b[1m4775/4775\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 66ms/step\n",
      "\n",
      "=== Results ===\n",
      "        Model Accuracy    F1 ROC_AUC\n",
      "0  LSTM_Focal     None  None    None\n",
      "\u001b[1m4775/4775\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 58ms/step\n",
      "\n",
      "Saved predictions to submission_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ==============================\n",
    "# Main Execution\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    train_df = pd.read_csv(r\"Dataset\\train.csv\")\n",
    "    test_df = pd.read_csv(r\"Dataset\\test.csv\")\n",
    "\n",
    "   \n",
    "    TEXT_COL = \"comment_text\"    \n",
    "    TARGET_COL = \"psychotic_depression\"   \n",
    "    ID_COL = \"id\" if \"id\" in test_df.columns else None\n",
    "\n",
    "    # Preprocess datasets\n",
    "    train_df = preprocess_dataframe(train_df, TEXT_COL, remove_value='\"')\n",
    "    test_df = preprocess_dataframe(test_df, TEXT_COL, remove_value='\"')\n",
    "\n",
    "    print(\"Train shape after cleaning:\", train_df.shape)\n",
    "    print(\"Test shape after cleaning:\", test_df.shape)\n",
    "\n",
    "    # Prepare X, y\n",
    "    X = train_df[TEXT_COL].astype(str).values\n",
    "    y = train_df[TARGET_COL].values\n",
    "\n",
    "    if y.dtype == 'O':\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    X_test = test_df[TEXT_COL].astype(str).values\n",
    "    y_test = test_df[TARGET_COL].values if TARGET_COL in test_df else None\n",
    "\n",
    "    clf = TextClassifier(max_features=20000, max_length=200)\n",
    "\n",
    "    results, X_test_pad = clf.train_lstm_models(\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "        balance_method=\"balanced_weights\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Results ===\")\n",
    "    print(results)\n",
    "\n",
    "    # Save predictions from best model\n",
    "    best_model_name = results.sort_values(\"F1\" if \"F1\" in results else \"ROC_AUC\",\n",
    "                                          ascending=False).iloc[0][\"Model\"]\n",
    "    best_model = clf.models[best_model_name]\n",
    "\n",
    "    test_probs = best_model.predict(X_test_pad).flatten()\n",
    "    test_preds = (test_probs > 0.5).astype(int)\n",
    "\n",
    "    output = pd.DataFrame({\n",
    "        ID_COL if ID_COL else \"index\": test_df[ID_COL] if ID_COL else range(len(test_df)),\n",
    "        \"prediction\": test_preds\n",
    "    })\n",
    "\n",
    "    output.to_csv(\"output.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
