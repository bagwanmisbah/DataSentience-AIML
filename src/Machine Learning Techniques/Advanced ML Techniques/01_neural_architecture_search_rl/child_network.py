"""
Dynamic Child Network Builder for Neural Architecture Search
This module builds and trains neural networks based on architectures generated by the controller.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Tuple, Optional
import numpy as np


class DynamicLayer(nn.Module):
    """Dynamic layer that can be configured based on architecture actions."""
    
    def __init__(self, layer_type: str, in_channels: int, out_channels: int, **kwargs):
        super(DynamicLayer, self).__init__()
        self.layer_type = layer_type
        self.in_channels = in_channels
        self.out_channels = out_channels
        
        # Build the layer based on type
        self.layer = self._build_layer(**kwargs)
        
    def _build_layer(self, **kwargs):
        """Build the actual layer based on layer type."""
        if self.layer_type == 'CONV_3x3':
            return nn.Conv2d(self.in_channels, self.out_channels, 3, padding=1)
        elif self.layer_type == 'CONV_5x5':
            return nn.Conv2d(self.in_channels, self.out_channels, 5, padding=2)
        elif self.layer_type == 'CONV_7x7':
            return nn.Conv2d(self.in_channels, self.out_channels, 7, padding=3)
        elif self.layer_type == 'MAX_POOL_3x3':
            return nn.MaxPool2d(3, stride=1, padding=1)
        elif self.layer_type == 'AVG_POOL_3x3':
            return nn.AvgPool2d(3, stride=1, padding=1)
        elif self.layer_type == 'SEPARABLE_CONV_3x3':
            return nn.Sequential(
                nn.Conv2d(self.in_channels, self.in_channels, 3, padding=1, groups=self.in_channels),
                nn.Conv2d(self.in_channels, self.out_channels, 1)
            )
        elif self.layer_type == 'SEPARABLE_CONV_5x5':
            return nn.Sequential(
                nn.Conv2d(self.in_channels, self.in_channels, 5, padding=2, groups=self.in_channels),
                nn.Conv2d(self.in_channels, self.out_channels, 1)
            )
        elif self.layer_type == 'DILATED_CONV_3x3':
            return nn.Conv2d(self.in_channels, self.out_channels, 3, padding=2, dilation=2)
        elif self.layer_type == 'DILATED_CONV_5x5':
            return nn.Conv2d(self.in_channels, self.out_channels, 5, padding=4, dilation=2)
        elif self.layer_type == 'BATCH_NORM':
            return nn.BatchNorm2d(self.in_channels)
        elif self.layer_type == 'DROPOUT':
            return nn.Dropout2d(0.5)
        elif self.layer_type == 'RELU':
            return nn.ReLU()
        elif self.layer_type == 'SIGMOID':
            return nn.Sigmoid()
        elif self.layer_type == 'TANH':
            return nn.Tanh()
        else:
            raise ValueError(f"Unknown layer type: {self.layer_type}")
    
    def forward(self, x):
        return self.layer(x)


class ChildNetwork(nn.Module):
    """
    Dynamic neural network that can be built from architecture actions.
    """
    
    def __init__(self, 
                 input_shape: Tuple[int, int, int],
                 num_classes: int,
                 architecture_actions: List[int],
                 max_channels: int = 512):
        super(ChildNetwork, self).__init__()
        
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.architecture_actions = architecture_actions
        self.max_channels = max_channels
        
        # Build the network
        self.layers = nn.ModuleList()
        self.skip_connections = []
        self.current_channels = input_shape[0]
        self.feature_maps = []
        
        self._build_network()
        
    def _build_network(self):
        """Build the network based on architecture actions."""
        # Architecture vocabulary mapping
        vocab = {
            2: 'CONV_3x3', 3: 'CONV_5x5', 4: 'CONV_7x7',
            5: 'MAX_POOL_3x3', 6: 'AVG_POOL_3x3',
            7: 'SEPARABLE_CONV_3x3', 8: 'SEPARABLE_CONV_5x5',
            9: 'DILATED_CONV_3x3', 10: 'DILATED_CONV_5x5',
            12: 'BATCH_NORM', 13: 'DROPOUT',
            14: 'RELU', 15: 'SIGMOID', 16: 'TANH'
        }
        
        layer_idx = 0
        for action in self.architecture_actions:
            if action in vocab:
                layer_type = vocab[action]
                
                # Determine output channels
                if 'CONV' in layer_type or 'SEPARABLE' in layer_type:
                    out_channels = min(self.current_channels * 2, self.max_channels)
                else:
                    out_channels = self.current_channels
                
                # Create layer
                layer = DynamicLayer(layer_type, self.current_channels, out_channels)
                self.layers.append(layer)
                
                # Update channel count
                if 'CONV' in layer_type or 'SEPARABLE' in layer_type:
                    self.current_channels = out_channels
                
                layer_idx += 1
                
                # Limit number of layers
                if layer_idx >= 20:
                    break
        
        # Add final classification layers
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.classifier = nn.Linear(self.current_channels, self.num_classes)
        
    def forward(self, x):
        """Forward pass through the dynamic network."""
        # Store intermediate feature maps for skip connections
        feature_maps = [x]
        
        # Pass through dynamic layers
        for i, layer in enumerate(self.layers):
            x = layer(x)
            
            # Store feature map for potential skip connections
            if i < len(feature_maps):
                feature_maps.append(x)
            
            # Apply skip connections if specified
            if i > 0 and i < len(self.architecture_actions):
                if self.architecture_actions[i] == 17:  # SKIP_CONNECTION
                    if i - 1 < len(feature_maps):
                        # Add skip connection
                        x = x + feature_maps[i - 1]
        
        # Global pooling and classification
        x = self.global_pool(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        
        return x
    
    def get_num_parameters(self) -> int:
        """Get the number of trainable parameters."""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def get_model_size_mb(self) -> float:
        """Get model size in megabytes."""
        param_size = 0
        for param in self.parameters():
            param_size += param.nelement() * param.element_size()
        buffer_size = 0
        for buffer in self.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()
        size_all_mb = (param_size + buffer_size) / 1024**2
        return size_all_mb


class ChildNetworkTrainer:
    """
    Trainer for child networks in NAS.
    """
    
    def __init__(self, 
                 model: ChildNetwork,
                 train_loader,
                 val_loader,
                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.1)
        
    def train_epoch(self) -> float:
        """Train for one epoch."""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(self.train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.criterion(output, target)
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
            
        self.scheduler.step()
        return total_loss / len(self.train_loader), correct / total
    
    def evaluate(self) -> Tuple[float, float]:
        """Evaluate the model on validation set."""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in self.val_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                total_loss += self.criterion(output, target).item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
        
        return total_loss / len(self.val_loader), correct / total
    
    def train_for_epochs(self, num_epochs: int = 10) -> Dict[str, List[float]]:
        """Train the model for specified number of epochs."""
        history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': []
        }
        
        for epoch in range(num_epochs):
            train_loss, train_acc = self.train_epoch()
            val_loss, val_acc = self.evaluate()
            
            history['train_loss'].append(train_loss)
            history['train_acc'].append(train_acc)
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)
            
            print(f'Epoch {epoch+1}/{num_epochs}:')
            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')
            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')
        
        return history


def build_child_network_from_actions(actions: List[int],
                                   input_shape: Tuple[int, int, int] = (3, 32, 32),
                                   num_classes: int = 10) -> ChildNetwork:
    """Convenience function to build a child network from actions."""
    return ChildNetwork(input_shape, num_classes, actions)


if __name__ == "__main__":
    # Example usage
    input_shape = (3, 32, 32)
    num_classes = 10
    
    # Sample architecture actions
    actions = [2, 14, 12, 5, 14, 3, 14, 12]  # CONV_3x3, RELU, BATCH_NORM, MAX_POOL, RELU, CONV_5x5, RELU, BATCH_NORM
    
    # Build child network
    child_net = build_child_network_from_actions(actions, input_shape, num_classes)
    
    print(f"Child Network Parameters: {child_net.get_num_parameters():,}")
    print(f"Model Size: {child_net.get_model_size_mb():.2f} MB")
    
    # Test forward pass
    x = torch.randn(1, *input_shape)
    output = child_net(x)
    print(f"Output shape: {output.shape}")
