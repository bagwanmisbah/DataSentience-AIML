{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "buyer-s-time.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "kylwjuZy0tGA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "uZccR7_L0tGi"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV,KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder , OneHotEncoder \n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "\n",
        "# np.sqrt(mean_squared_log_error(actual, predicted))\n",
        "\n",
        "import xgboost as xgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "62Y3acxI0tGl"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "\n",
        "\n",
        "#train_df =  pd.read_csv('C:\\Users\\nkdhe\\OneDrive\\Desktop\\Machine Learning A-Z Template/ParticipantData_BTPC/Train.csv')\n",
        "#test_df = pd.read_csv('C:\\Users\\nkdhe\\OneDrive\\Desktop\\Machine Learning A-Z Template/ParticipantData_BTPC/Test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDNRu8074KMQ"
      },
      "source": [
        "from google.colab import files\r\n",
        "uploaded = files.upload()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr6RQcmi4Ryr"
      },
      "source": [
        "from google.colab import files\r\n",
        "uploaded = files.upload()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ryd3hjf4SpP"
      },
      "source": [
        "train_df= pd.read_csv('Train.csv')\r\n",
        "test_df= pd.read_csv('Test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-HDGEiBK0tGm"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "i4tdtMmy0tGo"
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3EnjTpa80tGq"
      },
      "source": [
        "len(pd.unique(train_df['session_id']))==len(train_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "whAA5ycv0tGs"
      },
      "source": [
        "train = train_df.copy()\n",
        "test = test_df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYcKNG6r0tGt"
      },
      "source": [
        "# **BASIC EDA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NWXe96ek0tGu"
      },
      "source": [
        "train = train.drop(labels=['session_id','client_agent'],axis = 1)\n",
        "test = test.drop(labels=['session_id','client_agent'],axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09EYlsoK5CQR"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "id": "en6_Vg7P0tGv"
      },
      "source": [
        "train['device_details'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "id": "vtGfgFAG0tGw"
      },
      "source": [
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "id": "ZPSXqxA30tGx"
      },
      "source": [
        "train['session_number'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "id": "i2tUuziH0tGy"
      },
      "source": [
        "train[\"date\"] = pd.to_datetime(train['date'])\n",
        "test[\"date\"] = pd.to_datetime(test['date'])\n",
        "\n",
        "train[\"year\"] = train['date'].dt.year\n",
        "test[\"year\"] = test['date'].dt.year\n",
        "\n",
        "train[\"month\"] = train['date'].dt.month\n",
        "test[\"month\"] = test['date'].dt.month\n",
        "\n",
        "train['day']= train['date'].dt.day\n",
        "test[\"day\"] = test['date'].dt.day\n",
        "\n",
        "train['diff_date'] = ((train['date'] - pd.datetime(1970,1,1)).dt.total_seconds())/(10**6)\n",
        "test['diff_date'] = ((test['date'] - pd.datetime(1970,1,1)).dt.total_seconds())/(10**6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FuNzXJIc0tGz"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BwTF2VoM0tGz"
      },
      "source": [
        "l = pd.unique(train[\"device_details\"])\n",
        "device_encoding = {}\n",
        "x = 0\n",
        "for i in l:\n",
        "    if i not in device_encoding:\n",
        "        device_encoding[i] = x\n",
        "        x+=1\n",
        "print(device_encoding)\n",
        "\n",
        "train[\"device_encoding\"] = train[\"device_details\"].map(device_encoding)\n",
        "test[\"device_encoding\"] = test[\"device_details\"].map(device_encoding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ixPjxtZL0tG0"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0yFmew0L0tG1"
      },
      "source": [
        "train= train[pd.notnull(train[\"time_spent\"])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDZEHd0u0tG2"
      },
      "source": [
        "# **MODELLING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "abx91bv00tG2"
      },
      "source": [
        "cols = ['session_number','device_encoding','purchased','added_in_cart','checked_out','year','month','day','diff_date']\n",
        "#cols = ['session_number','purchased','added_in_cart','checked_out']\n",
        "X = train[cols]\n",
        "Y = train[\"time_spent\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "id": "oK_FGJim0tG3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "EVnRe3x_0tG3"
      },
      "source": [
        "x_tr,x_tst,y_tr,y_tst = train_test_split(X,Y,test_size=0.20,random_state=0)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hclUiDR7RV0b"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "h5yAaHzE0tG4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "id": "1yV8mz3B0tG5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vViNqptK0tG5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7_bKmXts0tG6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-n8xE240tG7"
      },
      "source": [
        "# **FILE CREATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8CTDrRr20tG7"
      },
      "source": [
        "sub = pd.read_csv('../input/machinehack-buyers-time-prediction-challenge/ParticipantData_BTPC/Sample Submission.csv')\n",
        "sub[\"time_spent\"]=abs(ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "R0S1MZqs0tG8"
      },
      "source": [
        "sub.to_csv('sub1_xgb.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH_ErvHY5fbe"
      },
      "source": [
        "# Neural Network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7PlnLoa5gsA"
      },
      "source": [
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEma7-yK5hRl"
      },
      "source": [
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import torch\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "### Load the data\r\n",
        "\r\n",
        "# First we load the entire CSV file into an m x 3\r\n",
        "\r\n",
        "\r\n",
        "# We extract all rows and the first 2 columns, and then transpose it\r\n",
        "x_dataset = x_tr\r\n",
        "\r\n",
        "# We extract all rows and the last column, and transpose it\r\n",
        "y_dataset = y_tr\r\n",
        "\r\n",
        "# And make a convenient variable to remember the number of input columns\r\n",
        "n = 4\r\n",
        "\r\n",
        "\r\n",
        "### Model definition ###\r\n",
        "\r\n",
        "# First we define the trainable parameters A and b \r\n",
        "A = torch.randn((1, n), requires_grad=True)\r\n",
        "b = torch.randn(1, requires_grad=True)\r\n",
        "\r\n",
        "# Then we define the prediction model\r\n",
        "def model(x_input):\r\n",
        "    return A.mm(x_input.float()) + b\r\n",
        "\r\n",
        "\r\n",
        "### Loss function definition ###\r\n",
        "\r\n",
        "def loss(y_predicted, y_target):\r\n",
        "    return ((y_predicted - y_target)**2).sum()\r\n",
        "\r\n",
        "### Training the model ###\r\n",
        "\r\n",
        "# Setup the optimizer object, so it optimizes a and b.\r\n",
        "optimizer = optim.Adam([A, b], lr=0.1)\r\n",
        "\r\n",
        "# Main optimization loop\r\n",
        "for t in range(2000):\r\n",
        "    # Set the gradients to 0.\r\n",
        "    optimizer.zero_grad()\r\n",
        "    # Compute the current predicted y's from x_dataset\r\n",
        "    y_predicted = model(x_dataset)\r\n",
        "    # See how far off the prediction is\r\n",
        "    current_loss = loss(y_predicted, y_dataset)\r\n",
        "    # Compute the gradient of the loss with respect to A and b.\r\n",
        "    current_loss.backward()\r\n",
        "    # Update A and b accordingly.\r\n",
        "    optimizer.step()\r\n",
        "    print(f\"t = {t}, loss = {current_loss}, A = {A.detach().numpy()}, b = {b.item()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfLeh-Eh_y2F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUeWisrcDpWe"
      },
      "source": [
        "# 2 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt4BzFy5G5_I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ven-kAJ0G6dk"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "x_val=x_tr.iloc[0:400, :]                                                        # Validation dataset\r\n",
        "y_val=y_tr.iloc[0:400]\r\n",
        "\r\n",
        "y_tr, y_tst, y_val = y_tr.astype(float), y_tst.astype(float), y_val.astype(float)\r\n",
        "#x_tr, x_tst, x_val = x_tr.astype(float), x_tst.astype(float), x_val.astype(float)\r\n",
        "\r\n",
        "# scaling \r\n",
        "#scaler = MinMaxScaler()\r\n",
        "#x_tr = scaler.fit_transform(x_tr)\r\n",
        "#x_val = scaler.transform(x_val)\r\n",
        "#x_tst = scaler.transform(x_tst)\r\n",
        "x_tr, y_tr = np.array(x_tr), np.array(y_tr)\r\n",
        "x_val, y_val = np.array(x_val), np.array(y_val)\r\n",
        "x_tst, y_tst = np.array(x_tst), np.array(y_tst)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZvDRjvhOhsF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlNaJxliEi3X"
      },
      "source": [
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from sklearn.preprocessing import MinMaxScaler    \r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import mean_squared_error, r2_score\r\n",
        "\r\n",
        "\r\n",
        "class RegressionDataset(Dataset):\r\n",
        "    \r\n",
        "    def __init__(self, X_data, y_data):\r\n",
        "        self.X_data = X_data\r\n",
        "        self.y_data = y_data\r\n",
        "        \r\n",
        "    def __getitem__(self, index):\r\n",
        "        return self.X_data[index], self.y_data[index]\r\n",
        "        \r\n",
        "    def __len__ (self):\r\n",
        "        return len(self.X_data)\r\n",
        "train_dataset = RegressionDataset(torch.from_numpy(x_tr).float(), torch.from_numpy(y_tr).float())\r\n",
        "val_dataset = RegressionDataset(torch.from_numpy(x_val).float(), torch.from_numpy(y_val).float())\r\n",
        "test_dataset = RegressionDataset(torch.from_numpy(x_tst).float(), torch.from_numpy(y_tst).float())\r\n",
        "\r\n",
        "EPOCHS = 400\r\n",
        "BATCH_SIZE = 64\r\n",
        "LEARNING_RATE = 0.1\r\n",
        "NUM_FEATURES = len(X.columns)\r\n",
        "\r\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\r\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=1)\r\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1)\r\n",
        "\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(device)\r\n",
        "\r\n",
        "class MultipleRegression(nn.Module):\r\n",
        "    def __init__(self, num_features):\r\n",
        "        super(MultipleRegression, self).__init__()\r\n",
        "        self.layer_1 = nn.Linear(num_features, 16)\r\n",
        "        self.layer_2 = nn.Linear(16, 32)\r\n",
        "        self.layer_3 = nn.Linear(32, 96)\r\n",
        "        self.layer_4 = nn.Linear(96, 64)\r\n",
        "        self.layer_5 = nn.Linear(64, 32)\r\n",
        "        self.layer_6 = nn.Linear(32, 16)\r\n",
        "        self.layer_out = nn.Linear(16,1)\r\n",
        "        self.relu = nn.Tanh()\r\n",
        "    def forward(self, inputs):\r\n",
        "      x = self.relu(self.layer_1(inputs))\r\n",
        "      x = self.relu(self.layer_2(x))\r\n",
        "      x = self.relu(self.layer_3(x))\r\n",
        "      x = self.relu(self.layer_4(x))\r\n",
        "      x = self.relu(self.layer_5(x))\r\n",
        "      x = self.relu(self.layer_6(x))\r\n",
        "      x = self.layer_out(x)\r\n",
        "      return (x)\r\n",
        "    def predict(self, test_inputs):\r\n",
        "      x = self.relu(self.layer_1(test_inputs))\r\n",
        "      x = self.relu(self.layer_2(x))\r\n",
        "      x = self.relu(self.layer_3(x))\r\n",
        "      x = self.relu(self.layer_4(x))\r\n",
        "      x = self.relu(self.layer_5(x))\r\n",
        "      x = self.relu(self.layer_6(x))\r\n",
        "      x = self.layer_out(x)\r\n",
        "      return (x)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "model = MultipleRegression(NUM_FEATURES)\r\n",
        "model.to(device)\r\n",
        "print(model)\r\n",
        "criterion = nn.MSELoss()\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\r\n",
        "\r\n",
        "\r\n",
        "loss_stats = {\r\n",
        "    'train': [],\r\n",
        "    \"val\": []\r\n",
        "}\r\n",
        "\r\n",
        "print(\"Begin training.\")\r\n",
        "for e in tqdm(range(1, EPOCHS+1)):\r\n",
        "    \r\n",
        "    # TRAINING\r\n",
        "    train_epoch_loss = 0\r\n",
        "model.train()\r\n",
        "for X_train_batch, y_train_batch in train_loader:\r\n",
        "\r\n",
        "  X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\r\n",
        "  optimizer.zero_grad()\r\n",
        "        \r\n",
        "  y_train_pred = model(X_train_batch)\r\n",
        "        \r\n",
        "  train_loss = criterion(y_train_pred, y_train_batch.unsqueeze(1))\r\n",
        "        \r\n",
        "  train_loss.backward()\r\n",
        "  optimizer.step()\r\n",
        "        \r\n",
        "  train_epoch_loss += train_loss.item()\r\n",
        "        \r\n",
        "        \r\n",
        "# VALIDATION\r\n",
        "with torch.no_grad():\r\n",
        "  \r\n",
        "  val_epoch_loss = 0\r\n",
        "  model.eval()\r\n",
        "  for X_val_batch, y_val_batch in val_loader:\r\n",
        "    X\r\n",
        "    X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\r\n",
        "            \r\n",
        "    y_val_pred = model(X_val_batch)\r\n",
        "                        \r\n",
        "    val_loss = criterion(y_val_pred, y_val_batch.unsqueeze(1))\r\n",
        "            \r\n",
        "    val_epoch_loss += val_loss.item()\r\n",
        "    loss_stats['train'].append(train_epoch_loss/len(train_loader))\r\n",
        "    loss_stats['val'].append(val_epoch_loss/len(val_loader))                              \r\n",
        "    \r\n",
        "    print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f}')    \r\n",
        "   \r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjGPLVhbK9ol"
      },
      "source": [
        "y_pred_list = []\r\n",
        "with torch.no_grad():\r\n",
        "    model.eval()\r\n",
        "    for X_batch, _ in test_loader:\r\n",
        "        X_batch = X_batch.to(device)\r\n",
        "        y_test_pred = model(X_batch)\r\n",
        "        y_pred_list.append(y_test_pred.cpu().numpy())\r\n",
        "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX-opT3vXjsg"
      },
      "source": [
        "score5=np.sqrt(mean_squared_log_error(y_tst,y_pred_list))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuKNzsk-YEvw"
      },
      "source": [
        "score5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KG1pGuVYF7k"
      },
      "source": [
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zufTkh9wLVHQ"
      },
      "source": [
        "\r\n",
        "test=np.array(test[cols])\r\n",
        "\r\n",
        "\r\n",
        "test_dataset = RegressionDataset(torch.from_numpy(test).float(),torch.from_numpy(test).float())\r\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGjfcl5cNyN6"
      },
      "source": [
        "y_pred_list = []\r\n",
        "with torch.no_grad():\r\n",
        "    model.eval()\r\n",
        "    for X_batch,_ in test_loader:\r\n",
        "        X_batch = X_batch.to(device)\r\n",
        "        y_test_pred = model(X_batch)\r\n",
        "        y_pred_list.append(y_test_pred.cpu().numpy())\r\n",
        "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u3QKCjgQ37U"
      },
      "source": [
        "sub = pd.read_csv('Sample Submission.csv')\r\n",
        "sub[\"time_spent\"]=y_pred_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXbaxoPPQ46Q"
      },
      "source": [
        "sub.to_csv('sub2_NN4.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3cClLpyQ5uU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kCSEmCYQ6l5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hcn8Es7OHXI"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi9DR1WcPC8A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}